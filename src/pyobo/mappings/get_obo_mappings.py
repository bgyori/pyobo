# -*- coding: utf-8 -*-

"""This script extracts xrefs inside a list of OBO files.

NOTE lots of information is available at:
https://ncim.nci.nih.gov/ncimbrowser/pages/source_help_info.jsf
"""

import json
import os
from collections import defaultdict
from typing import Mapping, Optional
from urllib.request import urlretrieve

import click
import networkx as nx
import obonet
import pandas as pd
from tqdm import tqdm

from pyobo.constants import OUTPUT_DIRECTORY
from pyobo.registries.registries import MIRIAM_CACHE_PATH, OLS_CACHE_PATH

HERE = os.path.abspath(os.path.dirname(__file__))
CURATED_DB_PATH = os.path.join(HERE, 'meta_mappings.json')

#: Keys are prefixes and values point to OBO URLs to download
OBO = {
    # High quality
    'hp': 'http://purl.obolibrary.org/obo/hp.obo',
    "chebi": "http://purl.obolibrary.org/obo/chebi.obo",
    "chiro": "https://raw.githubusercontent.com/obophenotype/chiro/master/chiro.obo",
    'doid': 'http://purl.obolibrary.org/obo/doid.obo',
    'efo': 'http://www.ebi.ac.uk/efo/efo.obo',
    "go": "http://purl.obolibrary.org/obo/go.obo",
    "obi": "http://purl.obolibrary.org/obo/obi.obo",
    # Others
    "pr": "http://purl.obolibrary.org/obo/pr.obo",
    "bto": "http://purl.obolibrary.org/obo/bto.obo",
    "cl": "http://purl.obolibrary.org/obo/cl.obo",
    # "clo":  # not distributed as OBO
    "cmo": "http://purl.obolibrary.org/obo/cmo.obo",
    "ecto": "http://purl.obolibrary.org/obo/ecto.obo",
    "exo": "http://purl.obolibrary.org/obo/exo.obo",
    "fbbt": "http://purl.obolibrary.org/obo/fbbt.obo",
    'mondo': 'http://purl.obolibrary.org/obo/mondo.obo',
    "mp": "http://purl.obolibrary.org/obo/mp.obo",
    "mpath": "https://raw.githubusercontent.com/PaulNSchofield/mpath/master/mpath.obo",
    'ncit': 'http://purl.obolibrary.org/obo/ncit.obo',
    "pato": "http://purl.obolibrary.org/obo/pato.obo",
    "peco": "http://purl.obolibrary.org/obo/peco.obo",
    "pw": "http://purl.obolibrary.org/obo/pw.obo",
    'symp': 'http://purl.obolibrary.org/obo/symp.obo',
    "to": "http://purl.obolibrary.org/obo/to.obo",
    "uberon": "http://purl.obolibrary.org/obo/uberon/basic.obo",
}

AUTOGENERATED = {
    "hgnc": None,
    "mirbase": None,
    "sgd": None,
    "hgnc.genefamily": None,
    "mgi": None,
    "rgd": None,
}
OBO.update(AUTOGENERATED)

with open(CURATED_DB_PATH) as file:
    CURATED_DB = json.load(file)

#: Xrefs starting with these prefixes will be ignored
XREF_PREFIX_BLACKLIST = set(CURATED_DB['blacklists']['prefix'])
XREF_SUFFIX_BLACKLIST = set(CURATED_DB['blacklists']['suffix'])
#: Xrefs matching these will be ignored
XREF_BLACKLIST = set(CURATED_DB['blacklists']['full'])

PREFIX_REMAP = CURATED_DB['remappings']['prefix']

ALLOWED_UNNORM = set(CURATED_DB['database'])

MIRIAM_DB = {}
if os.path.exists(MIRIAM_CACHE_PATH):
    with open(MIRIAM_CACHE_PATH) as file:
        MIRIAM_DB = json.load(file)

OLS_DB = {}
if os.path.exists(OLS_CACHE_PATH):
    with open(OLS_CACHE_PATH) as file:
        OLS_DB = json.load(file)


def get_namespace_synonyms() -> Mapping[str, str]:
    """Return a mapping from several variants of each synonym to the canonical namespace."""
    synonym_to_key = {}

    def _add_variety(_synonym, _target) -> None:
        synonym_to_key[_synonym] = _target
        synonym_to_key[_synonym.lower()] = _target
        synonym_to_key[_synonym.upper()] = _target
        synonym_to_key[_synonym.casefold()] = _target
        for x, y in [('_', ' '), (' ', '_'), (' ', '')]:
            synonym_to_key[_synonym.replace(x, y)] = _target
            synonym_to_key[_synonym.lower().replace(x, y)] = _target
            synonym_to_key[_synonym.upper().replace(x, y)] = _target
            synonym_to_key[_synonym.casefold().replace(x, y)] = _target

    for entry in MIRIAM_DB:
        prefix, name = entry['prefix'], entry['name']
        _add_variety(prefix, prefix)
        _add_variety(name, prefix)

    for entry in OLS_DB:
        ontology_id = entry['ontologyId']
        namespace = entry['config']['namespace']
        title = entry['config']['title']
        _add_variety(title, ontology_id)
        _add_variety(ontology_id, ontology_id)
        _add_variety(namespace, ontology_id)

    for key, values in CURATED_DB['database'].items():
        _add_variety(key, key)
        for synonym in values.get('synonyms', []):
            _add_variety(synonym, key)

    return synonym_to_key


SYNONYM_TO_KEY = get_namespace_synonyms()
UNHANDLED_NAMESPACES = defaultdict(list)
UBERON_UNHANDLED = defaultdict(list)


def normalize_namespace(namespace, node, xref=None) -> Optional[str]:
    """Normalize a namespace and return, if possible."""
    for t in (lambda x: x, str.lower, str.upper, str.casefold):
        namespace_transformed = t(namespace)
        if namespace_transformed in SYNONYM_TO_KEY:
            return SYNONYM_TO_KEY[namespace_transformed]

    if node.startswith('UBERON:'):  # uberon has tons of xrefs to anatomical features. skip them
        UBERON_UNHANDLED[namespace].append((node, xref))
        return

    UNHANDLED_NAMESPACES[namespace].append((node, xref))


def iterate_xrefs_from_graph(graph: nx.Graph, obo_key: str):
    for node, data in tqdm(graph.nodes(data=True), desc=f'Extracting {obo_key}'):
        node = node.strip()

        if node in XREF_BLACKLIST:
            continue

        # Skip node if it has a blacklisted prefix
        for prefix in XREF_PREFIX_BLACKLIST:
            if node.startswith(prefix):
                continue

        # Skip node if it has a blacklisted suffix
        for suffix in XREF_SUFFIX_BLACKLIST:
            if node.endswith(suffix):
                continue

        # Remap node's prefix (if necessary)
        for prefix, new_prefix in PREFIX_REMAP.items():
            if node.startswith(prefix):
                node = new_prefix + node[len(prefix):]

        try:
            head_ns, head_id = node.split(':', 1)
        except ValueError:  # skip nodes that don't look like normal CURIEs
            # print(f'skipping: {node}')
            continue

        norm_head_ns = normalize_namespace(head_ns, node, None)
        if not norm_head_ns:
            continue

        # TODO check if synonyms are also written like CURIEs,
        # ... not that they should be

        for xref in data.get('xref', []):
            xref = xref.strip()

            if (
                node == xref
                or any(xref.startswith(x) for x in XREF_PREFIX_BLACKLIST)
                or xref in XREF_BLACKLIST
                or ':' not in xref
            ):
                continue  # sometimes xref to self... weird

            for prefix, new_prefix in PREFIX_REMAP.items():
                if xref.startswith(prefix):
                    xref = new_prefix + xref[len(prefix):]

            split_space = ' ' in xref
            if split_space:
                _xref_split = xref.split(' ', 1)
                if _xref_split[1][0] not in {'"', '('}:
                    print(f'Problem with space in xref', node, xref)
                    continue
                xref = _xref_split[0]

            try:
                xref_ns, xref_id = xref.split(':', 1)
            except ValueError:
                if split_space:
                    print('problem splitting after space split', node, xref)
                else:
                    print('problem splitting', node, xref)

                continue

            norm_xref_ns = normalize_namespace(xref_ns, node, xref)
            if not norm_xref_ns:
                continue

            yield norm_head_ns, head_id, norm_xref_ns, xref_id, obo_key
            yield norm_xref_ns, xref_id, norm_head_ns, head_id, obo_key


@click.command()
@click.option('--directory', type=click.Path(dir_okay=True, file_okay=False), default=os.getcwd())
def main(directory):
    os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)

    xrefs = []
    for key, url in OBO.items():
        obo_path = os.path.join(OUTPUT_DIRECTORY, f'{key}.obo')
        if not os.path.exists(obo_path):
            print(f'downloading {url}')
            urlretrieve(url, obo_path)

        obo_pickle_path = os.path.join(OUTPUT_DIRECTORY, f'{key}.obo.pickle')
        if os.path.exists(obo_pickle_path):
            # click.secho(f'reading {obo_pickle_path}', fg='green', bold=True)
            g = nx.read_gpickle(obo_pickle_path)
        else:
            print(f'parsing {obo_path}')
            g = obonet.read_obo(obo_path)
            nx.write_gpickle(g, obo_pickle_path)

        xrefs.extend(iterate_xrefs_from_graph(g, key))

    export_sample_path = os.path.join(directory, f'xrefs_sample.tsv')

    df = pd.DataFrame(set(xrefs), columns=('head_ns', 'head_id', 'tail_ns', 'tail_id', 'source'))
    df = df.sort_values(['head_ns', 'head_id', 'tail_ns', 'tail_id', 'source'])

    df.to_csv(os.path.join(directory, f'xrefs.tsv'), sep='\t', index=False)
    df.to_csv(os.path.join(directory, f'xrefs.tsv.gz'), sep='\t', index=False)
    df.head().to_csv(export_sample_path, sep='\t', index=False)

    summary_path = os.path.join(directory, 'summary.tsv')
    summary_df = df.groupby(['source', 'tail_ns'])['head_ns'].count().reset_index().sort_values(['head_ns'],
                                                                                                ascending=False)
    summary_df.to_csv(summary_path, sep='\t', index=False)

    unmapped_path = os.path.join(directory, 'unmapped.tsv')
    with open(unmapped_path, 'w') as file:
        for namespace, items in tqdm(sorted(UNHANDLED_NAMESPACES.items()), desc='Outputting unmapped.tsv'):
            for node, xref in items:
                print(node, namespace, xref, file=file, sep='\t')


if __name__ == '__main__':
    main()
